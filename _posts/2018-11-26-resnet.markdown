---
layout: post
title:  "ResNet关键点总结 "
date:   2018-11-28 13:51:05 +0800
categories: jekyll update
---

#### 1、ResNet出现的背景是什么？
深度网络将低/中/高特征和分类器以端到端的形式进行集成，随着堆叠层数的增加，可以丰富不同“级别”的特征。但是随着网络深度的增加，也会产生一些问题：
* 梯度消失：阻碍了模型的收敛，这个问题基本上用BN得到了解决，BN保证几十层的网络能够收敛（SGD）。
* 退化问题：随着网络的加深，准确率开始饱和并且快速下降，并且在不同的数据集上都出现了退化现象。比如说56层的网络比20层的网络效果更差，但是退化问题并不是过拟合引起的，因为同时训练误差也会增大。如果是过拟合，应该是训练误差下，验证误差大。都如下图：

![resnet_02](/assets/image/resnet_02.png){:height="80%" width="80%"}

即使堆积的新层什么也不学，仅仅是复制浅层网络的特征，网络的性能也应该和浅层一样，而不是更差（有更高的训练误差）。所以何凯明等大神提出了Resnet，解决了上面所说的退化问题。

#### 2、ResNet解决了什么问题？
ResNet主要解决了退化问题：
* 引入了深度残差学习框架，相比于让一些堆叠层直接学习原始特征，而是让堆叠层去拟合残差映射。
* 增加了shortcut connections，ResNet主要是增加恒等映射，这样当残差为0时，此时堆积层做了恒等映射，至少网络的性能不会下降。之前网络是直接拟合H(x)，现在是拟合F(x)=H(x)-x，如下图，虽然学习出的结果是一样的，但是学习难度是不同的。并且恒等shortcut不增加额外参数也不增加计算复杂度。整个网络训练的时候仍然用SGD端到端反向传播调优。

![resnet_01](/assets/image/resnet_01.png){:height="50%" width="50%"}

退化问题的解决，同时也说明了之前求解器在估计恒等映射的时候是存在困难的。加上了残差学习，如果恒等映射是最优的，那么求解器会驱使多层非线性层的结果为0。所以ResNet的主要优势是：

* ResNet相对于Plain网络（只是简单堆叠的网络），更容易优化，Plain网络随着深度的增加，会有更高的训练误差。
* ResNet能从增加的深度中获得准确率的提升，Resnet产生的结果也刷新了当时的记录。
* ResNet扩展性强，当网络增加到100层甚至1000层的时候，仍然不会出现退化问题。

#### 3、Resnet网络结构是什么？

![resnet_05](/assets/image/resnet_05.png){:height="35%" width="35%"}
上图从左到右，分别是VGG19、34层Plain网络、34层ResNet：
* Plain网络的设计沿用了VGG的思路：
	* （1）对于相同的输出特征图尺寸具有相同数量的滤波器。
	* （2）如果特征图的尺寸减半（下采样，stride=2），则滤波器的数量加倍，以便保持每层的复杂度。
	* （3）但是网络设计上比VGG要简单，计算量大概是VGG的18%。
* ResNet在Plain的基础上增加了shortcut connection，如果同维度则对应相加，若非同维度，则有两种方案：
	* （1）仍为恒等映射，多余的维度用0进行填充。（可能会丢失信息）
	* （2）用1\*1的卷积进行维度匹配。
* 当网络不太深的时候（eg.18层），普通网络还是能找到理想的解，但是18层ResNet比18层Plain网络收敛更快。

#### 4、恒等映射具体如何实现？
根据上图可以知道每隔几层，运用一次残差学习，具体公式为：

![resnet_03](/assets/image/resnet_03.png){:height="25%" width="25%"}

其中F项代表残差映射要学习的部分，并且F=W2\*relu(W1x)，所以y=relu(W2\*relu(W1x)+x)。其中F+x表示的是shortcut connectio，现在分两种情况进行讨论：
 * F与x同维度时：是元素级别的对应相加。此时没有额外的参数和计算量产生。
 * F与x非同维度时：当改变channel数的时候，可以用一个线性映射Ws来匹配维度，如下图。并且这里的Ws只用来映射维度，因为恒等映射已经足够解决梯度退化的问题，并且非常的经济。

![resnet_04](/assets/image/resnet_04.png){:height="27%" width="27%"}

备注：其实在论文中，作者一共测试了三种shortcut:
* （A）零填充快捷连接，不会产生参数。
* （B）投影快捷连接，用于维度增加的情况，比如channel从64到128的时候，如果channel不变化，和(A)一致。
* （C）所有的连接都是投影连接。
通过看top-1和top-5 error，可知C好于B，B好于A，但是差异是较小的，说明投影连接对于解决退化问题的作用并不明显，所以一般不会选择C，因为会有太多的额外参数，增加了模型的复杂度和计算时间。
![resnet_07](/assets/image/resnet_07.png){:height="47%" width="47%"}

#### 5、瓶颈设计及网络拓展？
下面还有一个重要的点，就是瓶颈设计：
![resnet_06](/assets/image/resnet_06.png)
* 用右边3个卷积层代替左边的2个卷积层，3个卷积层是瓶颈模式的，即1\*1，3\*3，1\*1的结构，这里的1\*1主要是负责先降维再升维的作用，这样3\*3卷积核的参数量就会较少。
* 无参数恒等快捷连接对于瓶颈这种架构非常重要，如果将右图换成映射连接，连接的两个高维(256d)的两端，这样设计会大大增加模型size。
* 将34层ResNet中的卷积换成瓶颈模式的3层卷积网络，就变成了50层ResNet，并且采用上面的B方案进行围堵匹配映射，计算量为3.8bFLOPS（其中34层为3.6bFLOPS）
* 通过增添更多的3层瓶颈结构构件了101层和152层的ResNet，152层的计算量是11.3bFLOPS，仍然比VGG16的15.3bFLOPS的计算量少。
* 50/101/152层ResNet的效果好于34层ResNet，而且并未出现退化现象。

#### 6、小结模型效果？
ResNet不论是单模型还是组合模型，对比之前的网络都是最优的，有着很强的竞争力。

![resnet_09](/assets/image/resnet_09.png){:height="47%" width="47%"}

